{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOdjuZtrIm5blawtTxsi35t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agupte87/LearningPyspark/blob/master/Chapter3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEmJ2KAYSczP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2315a012-61b0-4c5e-86ac-17131c03f191"
      },
      "source": [
        "%cd ..\n",
        "%mkdir spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zxdVf2PQfT7"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I54Xf5OXQjFG",
        "outputId": "de66bd22-c378-4c0e-ddfa-6ab02188054b"
      },
      "source": [
        "!wget -P /spark/ https://apache.osuosl.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop3.2.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-21 18:38:01--  https://apache.osuosl.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop3.2.tgz\n",
            "Resolving apache.osuosl.org (apache.osuosl.org)... 64.50.236.52, 64.50.233.100, 140.211.166.134, ...\n",
            "Connecting to apache.osuosl.org (apache.osuosl.org)|64.50.236.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 224762914 (214M) [application/x-gzip]\n",
            "Saving to: ‘/spark/spark-3.0.3-bin-hadoop3.2.tgz’\n",
            "\n",
            "spark-3.0.3-bin-had 100%[===================>] 214.35M  71.1MB/s    in 3.0s    \n",
            "\n",
            "2021-07-21 18:38:04 (71.1 MB/s) - ‘/spark/spark-3.0.3-bin-hadoop3.2.tgz’ saved [224762914/224762914]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKp6fJVLQm_V"
      },
      "source": [
        "!tar xf /spark/spark-3.0.3-bin-hadoop3.2.tgz \n",
        "%mv spark-3.0.3-bin-hadoop3.2 /spark/\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0JIe2-tXEhX"
      },
      "source": [
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XQw4QA2W53J"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/spark/spark-3.0.3-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBFRhpg0Qr6N"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjsimwFaTBrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39c66e96-db70-4a44-e639-0e46447618ce"
      },
      "source": [
        "%cd /content/\n",
        "%mkdir /chapterData \n",
        "%cd /chapterData\n",
        "%mkdir retail-data\n",
        "%cd retail-data/\n",
        "%mkdir by-day\n",
        "%cd by-day"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/chapterData\n",
            "/chapterData/retail-data\n",
            "/chapterData/retail-data/by-day\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wmos7dt3dJ2E",
        "outputId": "de46d7a2-b7b3-4506-c072-415fa748f3be"
      },
      "source": [
        "%pwd\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/chapterData/retail-data/by-day'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe_baUzB4pR-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c097d60-0c73-4b06-a0df-2d01ec9abb3e"
      },
      "source": [
        "!wget -P  / 'https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/by-day/2010-12-05.csv'\n",
        "!wget -P  / 'https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/by-day/2010-12-06.csv'\n",
        "!wget -P  / 'https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/by-day/2010-12-07.csv'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-21 21:04:59--  https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/by-day/2010-12-05.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 246056 (240K) [text/plain]\n",
            "Saving to: ‘2010-12-05.csv’\n",
            "\n",
            "010-12-05.csv       100%[===================>] 240.29K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-07-21 21:04:59 (7.78 MB/s) - ‘2010-12-05.csv’ saved [246056/246056]\n",
            "\n",
            "--2021-07-21 21:04:59--  https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/by-day/2010-12-06.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 339039 (331K) [text/plain]\n",
            "Saving to: ‘2010-12-06.csv’\n",
            "\n",
            "010-12-06.csv       100%[===================>] 331.09K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-07-21 21:04:59 (9.82 MB/s) - ‘2010-12-06.csv’ saved [339039/339039]\n",
            "\n",
            "--2021-07-21 21:05:00--  https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/by-day/2010-12-07.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 255832 (250K) [text/plain]\n",
            "Saving to: ‘2010-12-07.csv’\n",
            "\n",
            "010-12-07.csv       100%[===================>] 249.84K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-07-21 21:05:00 (7.82 MB/s) - ‘2010-12-07.csv’ saved [255832/255832]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGVjVYqVRgAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f4e4aa0-b8ce-4e69-c80a-f1dd080e5a20"
      },
      "source": [
        "\n",
        "!wget -P  / 'https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/by-day/2010-12-01.csv'\n",
        "!wget -P  / 'https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/by-day/2010-12-02.csv'\n",
        "!wget -P  / 'https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/by-day/2010-12-03.csv'\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-21 18:38:18--  https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/by-day/2010-12-01.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 275001 (269K) [text/plain]\n",
            "Saving to: ‘2010-12-01.csv’\n",
            "\n",
            "010-12-01.csv       100%[===================>] 268.56K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-07-21 18:38:18 (8.50 MB/s) - ‘2010-12-01.csv’ saved [275001/275001]\n",
            "\n",
            "--2021-07-21 18:38:19--  https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/by-day/2010-12-02.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191826 (187K) [text/plain]\n",
            "Saving to: ‘2010-12-02.csv’\n",
            "\n",
            "010-12-02.csv       100%[===================>] 187.33K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-07-21 18:38:19 (7.11 MB/s) - ‘2010-12-02.csv’ saved [191826/191826]\n",
            "\n",
            "--2021-07-21 18:38:19--  https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/by-day/2010-12-03.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 190700 (186K) [text/plain]\n",
            "Saving to: ‘2010-12-03.csv’\n",
            "\n",
            "010-12-03.csv       100%[===================>] 186.23K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-07-21 18:38:19 (7.74 MB/s) - ‘2010-12-03.csv’ saved [190700/190700]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNPnnlShgSdy"
      },
      "source": [
        "%mv /chapterData /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECJS1OI4Stfq"
      },
      "source": [
        "**Prepping data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wify4dr8SF7h"
      },
      "source": [
        "staticDF = spark.read.format(\"csv\")\\\n",
        ".option(\"inferSchema\", \"true\")\\\n",
        ".option(\"header\", \"true\")\\\n",
        ".load(\"/content/chapterData/retail-data/by-day/*.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b73AFDqerQOh"
      },
      "source": [
        "staticSchema = staticDF.schema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUE5HtjUZc2l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb0c9d8a-743c-4797-be16-8b52fa285723"
      },
      "source": [
        "staticDF.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
            "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
            "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
            "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
            "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
            "|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n",
            "|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n",
            "|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
            "|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
            "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkE2TPwbohnn"
      },
      "source": [
        "**Static DF example to learn more about streaming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8oKWUmGcY2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0db450ba-51fe-46ae-dee3-9e4b491a0262"
      },
      "source": [
        "dayOfLargePurchase = staticDF.groupby(\"InvoiceDate\").sum(\"UnitPrice\").withColumnRenamed(\"sum(UnitPrice)\", \"SumOFUnitPrice\")\n",
        "dayOfLargePurchase.sort(dayOfLargePurchase.InvoiceDate.desc()).take(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(InvoiceDate='2010-12-03 17:28:00', SumOFUnitPrice=92.47),\n",
              " Row(InvoiceDate='2010-12-03 17:21:00', SumOFUnitPrice=1.45),\n",
              " Row(InvoiceDate='2010-12-03 17:20:00', SumOFUnitPrice=153.11),\n",
              " Row(InvoiceDate='2010-12-03 16:50:00', SumOFUnitPrice=0.0),\n",
              " Row(InvoiceDate='2010-12-03 16:45:00', SumOFUnitPrice=7.5),\n",
              " Row(InvoiceDate='2010-12-03 16:44:00', SumOFUnitPrice=24.599999999999998),\n",
              " Row(InvoiceDate='2010-12-03 16:37:00', SumOFUnitPrice=0.0),\n",
              " Row(InvoiceDate='2010-12-03 16:36:00', SumOFUnitPrice=0.0),\n",
              " Row(InvoiceDate='2010-12-03 16:35:00', SumOFUnitPrice=37.41),\n",
              " Row(InvoiceDate='2010-12-03 16:21:00', SumOFUnitPrice=60.540000000000006)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQgeW6D6fr0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac4bc22-1c10-42ea-fc6e-0dcbcff1bd16"
      },
      "source": [
        "from pyspark.sql.functions import window , column , desc , col \n",
        "staticDataFrame = staticDF.selectExpr(\"CustomerID\", \"(UnitPrice*Quantity) as total_purchase_value\", \"InvoiceDate\").groupBy(\\\n",
        "    col(\"CustomerId\"),\\\n",
        "    window(col(\"InvoiceDate\"),\"1 day\"))\\\n",
        ".sum(\"total_purchase_value\")\\\n",
        ".withColumnRenamed(\"sum(total_purchase_value)\", \"total_purchase_value\")\\\n",
        ".show(10)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+--------------------+\n",
            "|CustomerId|              window|total_purchase_value|\n",
            "+----------+--------------------+--------------------+\n",
            "|   13408.0|[2010-12-01 00:00...|  1024.6800000000003|\n",
            "|   17460.0|[2010-12-01 00:00...|                19.9|\n",
            "|   15235.0|[2010-12-01 00:00...|                79.5|\n",
            "|   13295.0|[2010-12-02 00:00...|               -3.25|\n",
            "|   12841.0|[2010-12-03 00:00...|              294.25|\n",
            "|   12779.0|[2010-12-03 00:00...|              248.16|\n",
            "|   14156.0|[2010-12-02 00:00...|                -7.5|\n",
            "|   13520.0|[2010-12-03 00:00...|               303.4|\n",
            "|   18239.0|[2010-12-02 00:00...|   438.0999999999999|\n",
            "|   17905.0|[2010-12-01 00:00...|  201.74999999999997|\n",
            "+----------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6A6ANB0ooC0"
      },
      "source": [
        "**Learning Streaming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2NJ4wN6qyBr"
      },
      "source": [
        "readData = spark.readStream.schema(staticSchema)\\\n",
        ".option(\"maxFilesTrigger\",1)\\\n",
        ".option(\"header\", \"true\")\\\n",
        ".format(\"csv\")\\\n",
        ".load(\"/content/chapterData/retail-data/by-day/*.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsHnrUZ6rwLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cd80f8e-9537-4e17-c6ce-3be9f57d915f"
      },
      "source": [
        "readData.isStreaming"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9UiKXCer7p8"
      },
      "source": [
        "purchaseCustomerPerHour = readData.selectExpr(\"CustomerId\", \"(UnitPrice*Quantity) as Total_Cost\", \"InvoiceDate\")\\\n",
        ".groupBy(col(\"CustomerId\")\\\n",
        ",window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
        ".sum(\"Total_Cost\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT8LXnpatHE9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "b8c8935a-63ad-4afe-bee3-e0808e3abdf7"
      },
      "source": [
        "purchaseCustomerPerHr= purchaseCustomerPerHour.writeStream\\\n",
        ".format(\"memory\")\\\n",
        ".queryName(\"Customer_Purchase\")\\\n",
        ".outputMode(\"complete\")\\\n",
        ".start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-c7a4bbfb424a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpurchaseCustomerPerHr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpurchaseCustomerPerHour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Customer_Purchase\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/spark/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot start query with name Customer_Purchase as a query with that name is already active in this SparkSession"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3D5ZSrhy0Du",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "764283b6-25e8-4bfb-b626-73979a22a5f7"
      },
      "source": [
        "spark.sql(\"select * from Customer_Purchase\").show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+------------------+\n",
            "|CustomerId|              window|   sum(Total_Cost)|\n",
            "+----------+--------------------+------------------+\n",
            "|   17460.0|[2010-12-01 00:00...|              19.9|\n",
            "|   13408.0|[2010-12-01 00:00...|1024.6800000000003|\n",
            "|   15235.0|[2010-12-01 00:00...|              79.5|\n",
            "|   12841.0|[2010-12-03 00:00...|            294.25|\n",
            "|   13295.0|[2010-12-02 00:00...|             -3.25|\n",
            "|   12779.0|[2010-12-03 00:00...|            248.16|\n",
            "|   14156.0|[2010-12-02 00:00...|              -7.5|\n",
            "|   13520.0|[2010-12-03 00:00...|             303.4|\n",
            "|   18239.0|[2010-12-02 00:00...| 438.0999999999999|\n",
            "|   12712.0|[2010-12-03 00:00...|175.53000000000003|\n",
            "+----------+--------------------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGf9XqJaZqnL"
      },
      "source": [
        "**Learning about Machine Learning!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g3SulOeWnMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df8ed11-4e1a-4a71-93d2-fe1d793ff6b8"
      },
      "source": [
        "staticDF.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- InvoiceNo: string (nullable = true)\n",
            " |-- StockCode: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- InvoiceDate: string (nullable = true)\n",
            " |-- UnitPrice: double (nullable = true)\n",
            " |-- CustomerID: double (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S2pBZLTZpaS"
      },
      "source": [
        "from pyspark.sql.functions import date_format, col\n",
        "preppedDateData  = staticDF.na.fill(0).withColumn(\"day_of_week\", \\\n",
        "                               date_format(col(\"InvoiceDate\"), \"EEEE\"))\\\n",
        "                               .coalesce(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1sxU8o6liPs",
        "outputId": "5f685050-6a54-4339-ecc0-21d1203339c6"
      },
      "source": [
        "#staticDF.filter(\"InvoiceDate is null\").show(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
            "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
            "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
            "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbpaN8ASqzrJ"
      },
      "source": [
        "**Checking the various dates to create train and test sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMQAvroPmqvj",
        "outputId": "1b70d14b-55e0-477a-de16-bf1318bad7e4"
      },
      "source": [
        "preppedDateData.select(\"InvoiceDate\")\\\n",
        ".distinct()\\\n",
        ".show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+\n",
            "|        InvoiceDate|\n",
            "+-------------------+\n",
            "|2010-12-02 11:45:00|\n",
            "|2010-12-02 18:49:00|\n",
            "|2010-12-01 17:29:00|\n",
            "|2010-12-02 18:45:00|\n",
            "|2010-12-03 12:28:00|\n",
            "|2010-12-02 15:26:00|\n",
            "|2010-12-01 09:34:00|\n",
            "|2010-12-02 09:43:00|\n",
            "|2010-12-02 15:25:00|\n",
            "|2010-12-03 09:31:00|\n",
            "+-------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNiUcXXvnOxV"
      },
      "source": [
        "trainingData = preppedDateData.where(\"InvoiceDate < '2010-12-03'\")\n",
        "testData = preppedDateData.where(\"InvoiceDate >= '2010-12-03'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7faex_ynbKq",
        "outputId": "4ae14f70-c5e3-424a-ed12-c41ab82f32f0"
      },
      "source": [
        "trainingData.count()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3108"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akdjLlQcpJII",
        "outputId": "39b17bca-f09e-4505-ecb5-e2f34092f58d"
      },
      "source": [
        "testData.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4311"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWbZFT3_t72a"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "indexer = StringIndexer(inputCol=\"day_of_week\", outputCol=\"day_of_week_index\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7JpZcaGwCMu"
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder \n",
        "encoder = OneHotEncoder(inputCol=\"day_of_week_index\", outputCol=\"day_of_week_encoded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7NPRndI1iEG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "936da424-9efd-4b44-c7e9-043bf127b3a5"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "vectorAssembler = VectorAssembler(outputCol=\"features\")\n",
        "vectorAssembler.setInputCols([\"UnitPrice,Quantity,day_of_week_encoded\"])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorAssembler_769a7088201f"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9bqoN7y3PhD"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "transformationPipeline = Pipeline().setStages([indexer,encoder,vectorAssembler])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taPBy0wc4IY2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "0fd49c6f-42c0-4ac0-855e-37fe7e7d16c6"
      },
      "source": [
        "fittedPipeline = transformationPipeline.fit(trainingData)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-154062cae9fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfittedPipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformationPipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/spark/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: The input column day_of_week_index should have at least two distinct values."
          ]
        }
      ]
    }
  ]
}